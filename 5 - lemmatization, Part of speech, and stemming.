import string


#1. POS Tagging
def simple_pos_tagger(word):
  if word.endswith("ing") or word.endswith("ed"): return "VERB"
  elif word.endswith("ly"): return "ADV"
  elif word.endswith("ous") or word.endswith("ful") or word.endswith("able"): return "ADJ"
  elif word in ["he", "she", "it", "they", "we", "i"]: return "PRON"
  elif word in ["and", "or", "but"]: return "CONJ"
  else: return "NOUN"

#2. Stemming
def simple_stemmer(word):
  suffixes = ["ing", "ly", "ed", "s", "es"]
  for suffix in suffixes:
    if word.endswith(suffix) and len(word) > len(suffix) + 2:
      return word[:-len(suffix)]
  return word


#3. Lemmatizer
def simple_lemmatizer (word, pos):
  lemmas = {
    "was": "be",
    "were": "be",
    "feet": "foot",
    "better": "good",
    "mice": "mouse",
    "ran": "run",
    "ate": "eat"
  }
  if word in lemmas:
    return lemmas[word]
  if pos == "VERB":
    return simple_stemmer(word)
  return word

# Custom tokenizer (no NLTK)
def simple_tokenizer(text):
  text = text.translate(str.maketrans('', '', string.punctuation)) # remove punctuation
  return text.lower().split()


# Testing
sentence = input("Enter a sentence: ")  # He was running quickly with better shoes.
tokens = simple_tokenizer (sentence)

print("\nResults:\n" + "-"*30)
for word in tokens:
  pos = simple_pos_tagger(word)
  stem = simple_stemmer(word)
  lemma = simple_lemmatizer(word, pos)
  print(f"word: {word: <12} POS: {pos: <5} Stem: {stem: <10} Lemma: {lemma}")
